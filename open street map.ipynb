{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# map information \n",
    "### The city I choose is Ann Arbor, MI,US . The map range is (-83.8724,42.2166,-83.6029,42.3301). The downloaded file is called 'map'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data audit\n",
    "use the iterative parsing to process the map file and\n",
    "find out not only what tags are there, but also how many, to get the\n",
    "feeling on how much of which data you can expect to have in the map.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file size is 85 MB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "file_path = 'map'\n",
    "b = os.path.getsize(file_path)\n",
    "print 'file size is {} MB'.format(b/(1024*1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tag collection result:\n",
      "{'bounds': 1,\n",
      " 'member': 9460,\n",
      " 'meta': 1,\n",
      " 'nd': 473164,\n",
      " 'node': 388849,\n",
      " 'note': 1,\n",
      " 'osm': 1,\n",
      " 'relation': 657,\n",
      " 'remark': 1,\n",
      " 'tag': 208224,\n",
      " 'way': 59064}\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.cElementTree as ET\n",
    "import pprint\n",
    "\n",
    "def count_tags(filename):\n",
    "       \n",
    "    tags = {}\n",
    "    for event,element in ET.iterparse(filename):\n",
    "        tag = element.tag\n",
    "        if tag not in tags.keys():\n",
    "            tags[tag] = 1\n",
    "        else:\n",
    "            tags[tag]+=1\n",
    "    return tags\n",
    "tags = count_tags('map')\n",
    "print 'tag collection result:'\n",
    "pprint.pprint(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Before you process the data and add it into your database, you should check the\n",
    "\"k\" value for each \"tag\" and see if there are any potential problems.\n",
    "\n",
    " we have a count of each of four tag categories in a dictionary:\n",
    "  \"lower\", for tags that contain only lowercase letters and are valid,\n",
    "  \"lower_colon\", for otherwise valid tags with a colon in their names,\n",
    "  \"problemchars\", for tags with problematic characters, and\n",
    "  \"other\", for other tags that do not fall into the other three categories.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lower': 141733, 'lower_colon': 61757, 'other': 4734, 'problemchars': 0}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "lower = re.compile(r'^([a-z]|_)*$')\n",
    "lower_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*$')\n",
    "problemchars = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "\n",
    "\n",
    "def key_type(element, keys):\n",
    "    if element.tag == \"tag\":\n",
    "        \n",
    "        if lower.match(element.attrib['k']):\n",
    "            keys[\"lower\"] = keys[\"lower\"] + 1\n",
    "        elif lower_colon.match(element.attrib['k']):\n",
    "            keys[\"lower_colon\"] = keys[\"lower_colon\"] + 1\n",
    "        elif problemchars.match(element.attrib['k']):\n",
    "            keys[\"problemchars\"] = keys[\"problemchars\"] + 1\n",
    "        else:\n",
    "            keys[\"other\"] = keys[\"other\"] + 1\n",
    "    return keys\n",
    "def process_map(filename):\n",
    "    keys = {\"lower\": 0, \"lower_colon\": 0, \"problemchars\": 0, \"other\": 0}\n",
    "    for _, element in ET.iterparse(filename):\n",
    "        keys = key_type(element, keys)\n",
    "\n",
    "    return keys\n",
    "keys = process_map('map')\n",
    "pprint.pprint(keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### find out how many unique users have contributed to the map in Ann Arbor!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "435\n"
     ]
    }
   ],
   "source": [
    "def process_map(filename):\n",
    "    users = set()\n",
    "    for _, element in ET.iterparse(filename):\n",
    "        if element.get('uid'):\n",
    "            users.add(element.get('uid'))\n",
    "\n",
    "    return users\n",
    "users = process_map('map')\n",
    "print len(users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# audit and update streetname\n",
    "We have offered normal street names in expected. the following work is mapping the problematic street name to the normal name. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Arcade': set(['Nickels Arcade']),\n",
      " 'Ave': set(['Jackson Ave']),\n",
      " 'Boardwalk': set(['Boardwalk']),\n",
      " 'Ct': set(['Armstrong Ct']),\n",
      " 'East': set(['Woodland Drive East']),\n",
      " 'Edenwood': set(['Edenwood']),\n",
      " 'Eisenhower': set(['West Eisenhower']),\n",
      " 'Highway': set(['South Industrial Highway']),\n",
      " 'North': set(['Village Circle North']),\n",
      " 'Plaza': set(['Jackson Plaza', 'Parkland Plaza']),\n",
      " 'Rd': set(['Jackson Rd']),\n",
      " 'South': set(['Village Circle South']),\n",
      " 'Way': set(['Carrot Way', 'Ember Way', 'Harbor Way', 'Victors Way']),\n",
      " 'road': set(['Carpenter road', 'Packard road'])}\n",
      "Village Circle North => Village Circle North\n",
      "Nickels Arcade => Nickels Arcade\n",
      "Village Circle South => Village Circle South\n",
      "Parkland Plaza => Parkland Square\n",
      "Jackson Plaza => Jackson Square\n",
      "Edenwood => Edenwood\n",
      "Jackson Rd => Jackson Road\n",
      "Boardwalk => Boardwalk\n",
      "South Industrial Highway => South Industrial Highway\n",
      "Ember Way => Ember Road\n",
      "Carrot Way => Carrot Road\n",
      "Victors Way => Victors Road\n",
      "Harbor Way => Harbor Road\n",
      "Jackson Ave => Jackson Avenue\n",
      "Woodland Drive East => Woodland Drive East\n",
      "West Eisenhower => West Eisenhower\n",
      "Carpenter road => Carpenter Road\n",
      "Packard road => Packard Road\n",
      "Armstrong Ct => Armstrong Court\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "OSMFILE = \"map\"\n",
    "street_type_re = re.compile(r'\\b\\S+\\.?$', re.IGNORECASE)\n",
    "\n",
    "\n",
    "expected = [\"Street\", \"Avenue\", \"Boulevard\", \"Drive\", \"Court\", \"Place\", \"Square\", \"Lane\", \"Road\", \n",
    "            \"Trail\", \"Parkway\", \"Commons\",'Circle']\n",
    "\n",
    "# UPDATE THIS VARIABLE\n",
    "mapping = { \"St\": \"Street\",\n",
    "            \"St.\": \"Street\",\n",
    "            \"Ave\": \"Avenue\",\n",
    "            \"Rd\": \"Road\",\n",
    "           \"road\":\"Road\",\n",
    "           'Plaza':\"Square\",\n",
    "           'Way':'Road',\n",
    "           'Ct':\"Court\"\n",
    "            }\n",
    "\n",
    "def audit_street_type(street_types, street_name):\n",
    "    m = street_type_re.search(street_name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        if street_type not in expected:\n",
    "            street_types[street_type].add(street_name)\n",
    "\n",
    "\n",
    "def is_street_name(elem):\n",
    "    return (elem.attrib['k'] == \"addr:street\")\n",
    "\n",
    "\n",
    "def audit(osmfile):\n",
    "    osm_file = open(osmfile, \"r\")\n",
    "    street_types = defaultdict(set)\n",
    "    for event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
    "\n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_street_name(tag):\n",
    "                    audit_street_type(street_types, tag.attrib['v'])\n",
    "    osm_file.close()\n",
    "    return street_types\n",
    "\n",
    "\n",
    "def update_name(name, mapping):\n",
    "\n",
    "    betterNameParts= []\n",
    "\n",
    "    name = name.split()\n",
    "    for index, part in enumerate(name):\n",
    "        if part in mapping.keys():\n",
    "            betterNameParts.append(mapping[part])\n",
    "        else:\n",
    "            betterNameParts.append(part)\n",
    "       \n",
    "    betterName = ' '.join(betterNameParts)\n",
    "\n",
    "    return betterName\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "st_types = audit(OSMFILE)\n",
    "\n",
    "pprint.pprint(dict(st_types))\n",
    "\n",
    "for st_type, ways in st_types.iteritems():\n",
    "    for name in ways:\n",
    "        better_name = update_name(name, mapping)\n",
    "        print name, \"=>\", better_name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing for Database\n",
    "## transfer xml data into csv + data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import codecs\n",
    "import pprint\n",
    "import re\n",
    "import xml.etree.cElementTree as ET\n",
    "\n",
    "\n",
    "\n",
    "OSM_PATH = \"map\"\n",
    "\n",
    "NODES_PATH = \"nodes.csv\"\n",
    "NODE_TAGS_PATH = \"nodes_tags.csv\"\n",
    "WAYS_PATH = \"ways.csv\"\n",
    "WAY_NODES_PATH = \"ways_nodes.csv\"\n",
    "WAY_TAGS_PATH = \"ways_tags.csv\"\n",
    "\n",
    "LOWER_COLON = re.compile(r'^([a-z]|_)+:([a-z]|_)+')\n",
    "PROBLEMCHARS = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Make sure the fields order in the csvs matches the column order in the sql table schema\n",
    "NODE_FIELDS = ['id', 'lat', 'lon', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "NODE_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_FIELDS = ['id', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "WAY_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_NODES_FIELDS = ['id', 'node_id', 'position']\n",
    "\n",
    "\n",
    "def shape_element(element, node_attr_fields=NODE_FIELDS, way_attr_fields=WAY_FIELDS,\n",
    "                  problem_chars=PROBLEMCHARS, default_tag_type='regular'):\n",
    "    \"\"\"Clean and shape node or way XML element to Python dict\"\"\"\n",
    "\n",
    "    node_attribs = {}\n",
    "    way_attribs = {}\n",
    "    way_nodes = []\n",
    "    tags = []  # Handle secondary tags the same way for both node and way elements\n",
    "\n",
    "    # the following code I have cited some from https://github.com/sfox1975/Udacity-DAND-Project-3/blob/master/data.py\n",
    "    if element.tag == 'node':\n",
    "\n",
    "            for node_field in node_attr_fields:\n",
    "                node_attribs[node_field] =element.attrib[node_field]\n",
    "\n",
    "            for tag in element.iter('tag'):\n",
    "                k = tag.attrib['k']\n",
    "                \n",
    "\n",
    "                # ignores tags containing problem characters in the k tag attribute:\n",
    "\n",
    "                if re.search(PROBLEMCHARS,k):\n",
    "                    continue\n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "                tag_dict = {}\n",
    "\n",
    "                tag_dict['id'] = node_attribs['id']\n",
    "\n",
    "                colon_find = re.split('[:]', k)\n",
    "\n",
    "                if len(colon_find) == 1:\n",
    "\n",
    "                    tag_dict['key'] = k\n",
    "                    tag_dict['type'] = 'regular'\n",
    "                    tag_dict['value'] = tag.attrib['v']\n",
    "\n",
    "                elif len(colon_find) == 2:\n",
    "\n",
    "                    tag_dict['key'] = colon_find[1]\n",
    "                    tag_dict['type'] = colon_find[0]\n",
    "                    # when the tag is street, we will update the street name \n",
    "                    if colon_find[1] == 'street':\n",
    "                        tag_dict['value'] = update_name(tag.attrib['v'], mapping)\n",
    "\n",
    "                elif len(colon_find) > 2:\n",
    "\n",
    "                    tag_dict['key'] = ':'.join(colon_find[1:])\n",
    "                    tag_dict['type'] = colon_find[0]\n",
    "                    tag_dict['value'] = tag.attrib['v']\n",
    "\n",
    "                \n",
    "\n",
    "                tags.append(tag_dict)\n",
    "\n",
    "            return {'node': node_attribs, 'node_tags': tags}\n",
    "\n",
    "    elif element.tag == 'way':\n",
    "\n",
    "        for way_field in way_attr_fields:\n",
    "            way_attribs[way_field] =element.attrib[way_field]\n",
    "\n",
    "        for tag in element.iter('tag'):\n",
    "            k = tag.attrib['k']\n",
    "\n",
    "            # ignores tags containing problem characters in the k tag attribute:\n",
    "\n",
    "            if re.search(PROBLEMCHARS,k):\n",
    "                print \"Problem character found - skipping element\"\n",
    "                continue\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "            tag_dict = {}\n",
    "\n",
    "            tag_dict['id'] = way_attribs['id']\n",
    "\n",
    "            colon_find = re.split('[:]', k)\n",
    "\n",
    "            if len(colon_find) == 1:\n",
    "\n",
    "                tag_dict['key'] = k\n",
    "                tag_dict['type'] = 'regular'\n",
    "                tag_dict['value'] = tag.attrib['v']\n",
    "\n",
    "            elif len(colon_find) == 2:\n",
    "\n",
    "                tag_dict['key'] = colon_find[1]\n",
    "                tag_dict['type'] = colon_find[0]\n",
    "                # when the tag is street, we will update the street name \n",
    "                if colon_find[1] == 'street':\n",
    "                    tag_dict['value'] = update_name(tag.attrib['v'], mapping)\n",
    "\n",
    "            elif len(colon_find) > 2:\n",
    "\n",
    "                tag_dict['key'] = ':'.join(colon_find[1:])\n",
    "                tag_dict['type'] = colon_find[0]\n",
    "                tag_dict['value'] = tag.attrib['v']\n",
    "\n",
    "            \n",
    "\n",
    "            tags.append(tag_dict)\n",
    "\n",
    "        n = 0\n",
    "        for nd in element.iter('nd'):\n",
    "\n",
    "            nd_dict = {}\n",
    "\n",
    "            nd_dict['id'] = way_attribs['id']\n",
    "            nd_dict['node_id'] = nd.attrib['ref']\n",
    "            nd_dict['position'] = n\n",
    "            way_nodes.append(nd_dict)\n",
    "            n+=1\n",
    "\n",
    "        return {'way': way_attribs, 'way_nodes': way_nodes, 'way_tags': tags}\n",
    "\n",
    "# ================================================== #\n",
    "#               Helper Functions                     #\n",
    "# ================================================== #\n",
    "def get_element(osm_file, tags=('node', 'way', 'relation')):\n",
    "    \"\"\"Yield element if it is the right type of tag\"\"\"\n",
    "\n",
    "    context = ET.iterparse(osm_file, events=('start', 'end'))\n",
    "    _, root = next(context)\n",
    "    for event, elem in context:\n",
    "        if event == 'end' and elem.tag in tags:\n",
    "            yield elem\n",
    "            root.clear()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class UnicodeDictWriter(csv.DictWriter, object):\n",
    "    \"\"\"Extend csv.DictWriter to handle Unicode input\"\"\"\n",
    "\n",
    "    def writerow(self, row):\n",
    "        super(UnicodeDictWriter, self).writerow({\n",
    "            k: (v.encode('utf-8') if isinstance(v, unicode) else v) for k, v in row.iteritems()\n",
    "        })\n",
    "\n",
    "    def writerows(self, rows):\n",
    "        for row in rows:\n",
    "            self.writerow(row)\n",
    "\n",
    "\n",
    "# ================================================== #\n",
    "#               Main Function                        #\n",
    "# ================================================== #\n",
    "def process_map(file_in, validate):\n",
    "    \"\"\"Iteratively process each XML element and write to csv(s)\"\"\"\n",
    "\n",
    "    with codecs.open(NODES_PATH, 'w') as nodes_file, \\\n",
    "         codecs.open(NODE_TAGS_PATH, 'w') as nodes_tags_file, \\\n",
    "         codecs.open(WAYS_PATH, 'w') as ways_file, \\\n",
    "         codecs.open(WAY_NODES_PATH, 'w') as way_nodes_file, \\\n",
    "         codecs.open(WAY_TAGS_PATH, 'w') as way_tags_file:\n",
    "\n",
    "        nodes_writer = UnicodeDictWriter(nodes_file, NODE_FIELDS)\n",
    "        node_tags_writer = UnicodeDictWriter(nodes_tags_file, NODE_TAGS_FIELDS)\n",
    "        ways_writer = UnicodeDictWriter(ways_file, WAY_FIELDS)\n",
    "        way_nodes_writer = UnicodeDictWriter(way_nodes_file, WAY_NODES_FIELDS)\n",
    "        way_tags_writer = UnicodeDictWriter(way_tags_file, WAY_TAGS_FIELDS)\n",
    "\n",
    "        nodes_writer.writeheader()\n",
    "        node_tags_writer.writeheader()\n",
    "        ways_writer.writeheader()\n",
    "        way_nodes_writer.writeheader()\n",
    "        way_tags_writer.writeheader()\n",
    "\n",
    "        \n",
    "\n",
    "        for element in get_element(file_in, tags=('node', 'way')):\n",
    "            el = shape_element(element)\n",
    "            if el:\n",
    "                \n",
    "                if element.tag == 'node':\n",
    "                    nodes_writer.writerow(el['node'])\n",
    "                    node_tags_writer.writerows(el['node_tags'])\n",
    "                elif element.tag == 'way':\n",
    "                    ways_writer.writerow(el['way'])\n",
    "                    way_nodes_writer.writerows(el['way_nodes'])\n",
    "                    way_tags_writer.writerows(el['way_tags'])\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Note: Validation is ~ 10X slower. For the project consider using a small\n",
    "    # sample of the map when validating.\n",
    "    process_map(OSM_PATH, validate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## use odo to transfer csv into sqlite3 database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wenhao\\Anaconda2\\lib\\site-packages\\odo\\backends\\pandas.py:94: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.\n",
      "You can access NaTType as type(pandas.NaT)\n",
      "  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import csv\n",
    "from odo import odo, resource, discover\n",
    "#  Specify file path\n",
    "#file_path = 'my_path/'\n",
    "# In this case 'my_path/' is a substitute for my real path\n",
    "\n",
    "# Specify csv file path and name\n",
    "#csv_path = file_path + 'ways.csv'\n",
    "csv_path1 = 'ways.csv'\n",
    "csv_path2 = 'ways_nodes.csv'\n",
    "csv_path3 = 'ways_tags.csv'\n",
    "csv_path4 = 'nodes.csv'\n",
    "csv_path5 = 'nodes_tags.csv'\n",
    "# Specify database name\n",
    "db_name = 'map.db'\n",
    "\n",
    "# Connect to new database\n",
    "conn = sqlite3.connect(db_name)\n",
    "\n",
    "# [2]\n",
    "\n",
    "# Use Odo to detect the shape and datatype of your csv:\n",
    "data_shape1 = discover(resource(csv_path1))\n",
    "data_shape2 = discover(resource(csv_path2))\n",
    "data_shape3 = discover(resource(csv_path3))\n",
    "data_shape4 = discover(resource(csv_path4))\n",
    "data_shape5 = discover(resource(csv_path5))\n",
    "# Ready in csv to new table called 'data' within database 'data.sqlite'\n",
    "odo(csv_path1, 'sqlite:///map.db::ways', dshape=data_shape1)\n",
    "odo(csv_path2, 'sqlite:///map.db::ways_nodes', dshape=data_shape2)\n",
    "odo(csv_path3, 'sqlite:///map.db::ways_tags', dshape=data_shape3)\n",
    "odo(csv_path4, 'sqlite:///map.db::nodes', dshape=data_shape4)\n",
    "odo(csv_path5, 'sqlite:///map.db::nodes_tags', dshape=data_shape5)\n",
    "# Close database\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# queries for created database\n",
    "###  check the tables' column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column name for ways tables:\n",
      "(0, u'id', u'BIGINT', 1, None, 0)\n",
      "(1, u'user', u'TEXT', 0, None, 0)\n",
      "(2, u'uid', u'BIGINT', 1, None, 0)\n",
      "(3, u'version', u'BIGINT', 1, None, 0)\n",
      "(4, u'changeset', u'BIGINT', 1, None, 0)\n",
      "(5, u'timestamp', u'DATETIME', 0, None, 0)\n",
      "column name for ways_nodes tables:\n",
      "(0, u'id', u'BIGINT', 1, None, 0)\n",
      "(1, u'node_id', u'BIGINT', 1, None, 0)\n",
      "(2, u'position', u'BIGINT', 1, None, 0)\n",
      "column name for ways_tags tables:\n",
      "(0, u'id', u'BIGINT', 1, None, 0)\n",
      "(1, u'key', u'TEXT', 0, None, 0)\n",
      "(2, u'value', u'TEXT', 0, None, 0)\n",
      "(3, u'type', u'TEXT', 0, None, 0)\n",
      "column name for nodes tables:\n",
      "(0, u'id', u'BIGINT', 1, None, 0)\n",
      "(1, u'lat', u'FLOAT', 1, None, 0)\n",
      "(2, u'lon', u'FLOAT', 1, None, 0)\n",
      "(3, u'user', u'TEXT', 0, None, 0)\n",
      "(4, u'uid', u'BIGINT', 1, None, 0)\n",
      "(5, u'version', u'BIGINT', 1, None, 0)\n",
      "(6, u'changeset', u'BIGINT', 1, None, 0)\n",
      "(7, u'timestamp', u'DATETIME', 0, None, 0)\n",
      "column name for nodes_tags tables:\n",
      "(0, u'id', u'BIGINT', 1, None, 0)\n",
      "(1, u'key', u'TEXT', 0, None, 0)\n",
      "(2, u'value', u'TEXT', 0, None, 0)\n",
      "(3, u'type', u'TEXT', 0, None, 0)\n"
     ]
    }
   ],
   "source": [
    "# Fetch records from either chinook.db\n",
    "db = sqlite3.connect(\"map.db\")\n",
    "c = db.cursor()\n",
    "QUERY = 'PRAGMA table_info(ways);'\n",
    "c.execute(QUERY)\n",
    "rows = c.fetchall()\n",
    "print \"column name for ways tables:\"\n",
    "for row in rows :\n",
    "    print row\n",
    "QUERY = 'PRAGMA table_info(ways_nodes);'\n",
    "c.execute(QUERY)\n",
    "rows = c.fetchall()\n",
    "print \"column name for ways_nodes tables:\"\n",
    "for row in rows :\n",
    "    print row\n",
    "QUERY = 'PRAGMA table_info(ways_tags);'\n",
    "c.execute(QUERY)\n",
    "rows = c.fetchall()\n",
    "print \"column name for ways_tags tables:\"\n",
    "for row in rows :\n",
    "    print row\n",
    "QUERY = 'PRAGMA table_info(nodes);'\n",
    "c.execute(QUERY)\n",
    "rows = c.fetchall()\n",
    "print \"column name for nodes tables:\"\n",
    "for row in rows :\n",
    "    print row\n",
    "QUERY = 'PRAGMA table_info(nodes_tags);'\n",
    "c.execute(QUERY)\n",
    "rows = c.fetchall()\n",
    "\n",
    "print \"column name for nodes_tags tables:\"\n",
    "for row in rows :\n",
    "    print row\n",
    "    \n",
    "\n",
    "\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###  check  the number of unique contributer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of unique contributors is:\n",
      "425\n"
     ]
    }
   ],
   "source": [
    "db = sqlite3.connect(\"map.db\")\n",
    "c = db.cursor()\n",
    "QUERY = 'select count(*) from (select distinct uid from ways union select distinct uid from nodes ) tmp'\n",
    "c.execute(QUERY)\n",
    "rows = c.fetchall()\n",
    "print \"total number of unique contributors is:\"\n",
    "for row in rows :\n",
    "    print row[0]\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check the number of ways and nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of ways is:\n",
      "59064\n",
      "total number of nodes is:\n",
      "388849\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "db = sqlite3.connect(\"map.db\")\n",
    "c = db.cursor()\n",
    "QUERY = 'select count(distinct id) from ways'\n",
    "c.execute(QUERY)\n",
    "rows = c.fetchall()\n",
    "print \"total number of ways is:\"\n",
    "for row in rows :\n",
    "    print row[0]\n",
    "QUERY = 'select count(distinct id) from nodes'\n",
    "c.execute(QUERY)\n",
    "rows = c.fetchall()\n",
    "print \"total number of nodes is:\"\n",
    "for row in rows :\n",
    "    print row[0]\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check the top ten most frequent type for nodes in Ann Arbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top ten most frequent type for nodes in Ann Arbor is:\n",
      "power 3516\n",
      "highway 3423\n",
      "name 1382\n",
      "amenity 940\n",
      "housenumber 932\n",
      "street 925\n",
      "created_by 886\n",
      "entrance 456\n",
      "shop 413\n",
      "barrier 356\n"
     ]
    }
   ],
   "source": [
    "db = sqlite3.connect(\"map.db\")\n",
    "c = db.cursor()\n",
    "QUERY = 'select key, count(*) from nodes, nodes_tags where nodes.id = nodes_tags.id group by key order by count(*) desc limit 10;'\n",
    "c.execute(QUERY)\n",
    "rows = c.fetchall()\n",
    "print \"top ten most frequent type for nodes in Ann Arbor is:\"\n",
    "for row in rows :\n",
    "    print row[0],row[1]\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  check the file size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_size information:\n",
      "1024\n",
      "page_count information:\n",
      "52268\n",
      "total database size is 51 MB\n"
     ]
    }
   ],
   "source": [
    "db = sqlite3.connect(\"map.db\")\n",
    "c = db.cursor()\n",
    "QUERY = 'pragma page_size;'\n",
    "c.execute(QUERY)\n",
    "\n",
    "rows = c.fetchall()\n",
    "print \"page_size information:\"\n",
    "for row in rows :\n",
    "    page_size =  row[0]\n",
    "    print row[0]\n",
    "QUERY = 'pragma page_count;'\n",
    "c.execute(QUERY)\n",
    "\n",
    "rows = c.fetchall()\n",
    "print \"page_count information:\"\n",
    "for row in rows :\n",
    "    page_count =  row[0]\n",
    "    print row[0]\n",
    "print 'total database size is {} MB'.format(page_size*page_count/(1024*1024))\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Other ideas about the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "top 3 contributor for nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 3 contributor for nodes in Ann Arbor is:\n",
      "mapper377 109416\n",
      "freebeer 29246\n",
      "IanH 28910\n"
     ]
    }
   ],
   "source": [
    "db = sqlite3.connect(\"map.db\")\n",
    "c = db.cursor()\n",
    "QUERY = 'select user, count(*) from nodes group by user order by count(*) desc limit 3;'\n",
    "c.execute(QUERY)\n",
    "rows = c.fetchall()\n",
    "print \"top 3 contributor for nodes in Ann Arbor is:\"\n",
    "for row in rows :\n",
    "    print row[0],row[1]\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top 10 frequent tag values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 30 frequent tag values for amenity in Ann Arbor is:\n",
      "restaurant 172\n",
      "bench 169\n",
      "cafe 65\n",
      "fast_food 55\n",
      "post_box 42\n",
      "fountain 40\n",
      "parking 33\n",
      "bicycle_parking 30\n",
      "bbq 23\n",
      "fuel 23\n"
     ]
    }
   ],
   "source": [
    "db = sqlite3.connect(\"map.db\")\n",
    "c = db.cursor()\n",
    "QUERY = 'select value, count(*) from nodes_tags where key = \"amenity\" group by value order by count(*) desc limit 10;'\n",
    "c.execute(QUERY)\n",
    "rows = c.fetchall()\n",
    "print \"top 30 frequent tag values for amenity in Ann Arbor is:\"\n",
    "for row in rows :\n",
    "    print row[0],row[1]\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 10 frequent tag values for shop in Ann Arbor is:\n",
      "clothes 59\n",
      "hairdresser 35\n",
      "convenience 27\n",
      "yes 17\n",
      "jewelry 14\n",
      "supermarket 13\n",
      "vacant 13\n",
      "beauty 12\n",
      "mobile_phone 12\n",
      "sports 10\n"
     ]
    }
   ],
   "source": [
    "db = sqlite3.connect(\"map.db\")\n",
    "c = db.cursor()\n",
    "QUERY = 'select value, count(*) from nodes_tags where key = \"shop\" group by value order by count(*) desc limit 10;'\n",
    "c.execute(QUERY)\n",
    "rows = c.fetchall()\n",
    "print \"top 10 frequent tag values for shop in Ann Arbor is:\"\n",
    "for row in rows :\n",
    "    print row[0],row[1]\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### find the value when tags' key = name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 10 frequent values for name in Ann Arbor is:\n",
      "#24 24\n",
      "Subway 9\n",
      "#7 8\n",
      "Starbucks 8\n",
      "#5 6\n",
      "Mailbox 6\n",
      "#66 5\n",
      "Espresso Royale 5\n",
      "Jimmy John's 5\n",
      "Sears 5\n"
     ]
    }
   ],
   "source": [
    "db = sqlite3.connect(\"map.db\")\n",
    "c = db.cursor()\n",
    "QUERY = 'select value, count(*) from nodes_tags where key = \"name\" group by value order by count(*) desc limit 10;'\n",
    "c.execute(QUERY)\n",
    "rows = c.fetchall()\n",
    "print \"top 10 frequent values for name in Ann Arbor is:\"\n",
    "for row in rows :\n",
    "    print row[0],row[1]\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### improvement adivce for the dataset\n",
    "here we find some problem , when the tags' key equals name, we find the name includes Subway, starbucks which belongs to the Key amenity; #24,#7 #5 belongs to the Key highway. Thus to better represent the data set, the tags whose key equals name should be classified to the general classes like amenity and highway"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### problems after the reclassification for the name tags :\n",
    "After switch these name to general one, like switch name tags Subway to amenity tags fastfood, there's no room to show this fastfood's name is Subway. Thus may cause information loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
